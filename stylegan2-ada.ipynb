{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNlv/N9ld2giij/+eVxVCnl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thomd/stylegan2-ada-experiments/blob/main/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pVgis8zfru1"
      },
      "source": [
        "StyleGAN2 works only with TensorFlow1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXetybFrfvxA",
        "outputId": "7ab1ba36-2ef4-4add-b5b3-7c6bdff7c285"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Thu Dec  2 14:26:59 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   68C    P8    32W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWLzvYQWgNXn"
      },
      "source": [
        "Mount your Drive to the Colab notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKuefE4kgQEL",
        "outputId": "13523f55-ab99-44ae-d2d3-0e90efc73628"
      },
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "\n",
        "content_path = Path('/').absolute() / 'content'\n",
        "drive_path = content_path / 'drive'\n",
        "drive.mount(str(drive_path))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "0hAgPJ3MirA_",
        "outputId": "050304ce-4722-431d-af22-90248d8067e8"
      },
      "source": [
        "str(content_path)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raeOZyKhgxd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "597da685-d26f-474e-a894-9489d387c236"
      },
      "source": [
        "#stylegan2_repo_url = 'https://github.com/NVlabs/stylegan2-ada'\n",
        "stylegan2_repo_url = 'https://github.com/dvschultz/stylegan2-ada'\n",
        "project_path = drive_path / 'MyDrive' / 'StyleGAN2-ADA'\n",
        "stylegan2_repo_path = project_path / 'stylegan2-ada'\n",
        "\n",
        "if not project_path.is_dir():\n",
        "    %mkdir \"{project_path}\"\n",
        "%cd \"{project_path}\"\n",
        "\n",
        "for dir in ['in', 'out', 'datasets', 'training']:\n",
        "    if not (project_path / dir).is_dir():\n",
        "        %mkdir {dir}\n",
        "\n",
        "if not (project_path / 'datasets' / 'source').is_dir():\n",
        "    %mkdir \"{project_path / 'datasets' / 'source'}\"\n",
        "\n",
        "# Clone or Update StyleGAN2-ADA\n",
        "if stylegan2_repo_path.is_dir():\n",
        "    !git -C \"{stylegan2_repo_path}\" fetch origin\n",
        "    !git -C \"{stylegan2_repo_path}\" checkout origin/main -- *.py\n",
        "else:\n",
        "    !git clone {stylegan2_repo_url}"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/StyleGAN2-ADA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtDuLRm8olnD"
      },
      "source": [
        "local_dataset_path = drive_path / 'MyDrive' / 'Datasets'\n",
        "if not local_dataset_path.is_dir():\n",
        "    %mkdir \"{local_dataset_path}\"\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZeaXz12Dxp8"
      },
      "source": [
        "## Data Upload\n",
        "\n",
        "upload training dataset as zip file into `Datasets` folder on Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKsWRC23D0WK"
      },
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(str(local_dataset_path / 'biked_small.zip'), 'r') as zip_ref:\n",
        "    zip_ref.extractall(str(local_dataset_path))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMKMhrM7IYcL"
      },
      "source": [
        "convert image dataset to a `.tfrecords` format that StyleGAN2-ADA can read"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOGQE8rhL6VJ",
        "outputId": "b7694c4b-d73e-4f76-a20b-132bf347f04f"
      },
      "source": [
        "stylegan2_repo_path"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/content/drive/MyDrive/StyleGAN2-ADA/stylegan2-ada')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4NEXsQDIfHz",
        "outputId": "0c781e66-f7e0-4639-e349-beea60107291"
      },
      "source": [
        "local_images_path = local_dataset_path / 'biked_small'\n",
        "local_tfr_images_path = local_dataset_path / 'biked_small_tfr'\n",
        "\n",
        "if local_tfr_images_path.is_dir():\n",
        "    print(f'Delete current dataset folder ({local_tfr_images_path}) to regenerate tfrecords')\n",
        "else:\n",
        "    %mkdir \"{local_tfr_images_path}\"\n",
        "    !python \"{stylegan2_repo_path / 'dataset_tool.py'}\" create_from_images \"{local_tfr_images_path}\" \"{local_images_path}\""
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading images from \"/content/drive/MyDrive/Datasets/biked_small\"\n",
            "Creating dataset \"/content/drive/MyDrive/Datasets/biked_small_tfr\"\n",
            "0 / 300\r/content/drive/MyDrive/StyleGAN2-ADA/stylegan2-ada/dataset_tool.py:97: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  'data': tf.train.Feature(bytes_list=tf.train.BytesList(value=[quant.tostring()]))}))\n",
            "Added 300 images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-mEDcKtss0D"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPBK_dYSMA15",
        "outputId": "1cc1ab61-e571-4c11-edee-ad7409cfbb16"
      },
      "source": [
        "!python \"{stylegan2_repo_path / 'train.py'}\" --help"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: train.py [-h] --outdir DIR [--gpus INT] [--snap INT] [--seed INT] [-n]\n",
            "                --data PATH [--res INT] [--mirror BOOL] [--mirrory BOOL]\n",
            "                [--use-raw BOOL] [--metrics LIST] [--metricdata PATH]\n",
            "                [--cfg {auto,11gb-gpu,11gb-gpu-complex,24gb-gpu,24gb-gpu-complex,48gb-gpu,48gb-2gpu,stylegan2,paper256,paper512,paper1024,cifar,cifarbaseline,aydao}]\n",
            "                [--lrate FLOAT] [--ttur BOOL] [--gamma FLOAT] [--nkimg INT]\n",
            "                [--kimg INT] [--topk FLOAT] [--aug {noaug,ada,fixed,adarv}]\n",
            "                [--p FLOAT] [--target TARGET] [--initstrength INITSTRENGTH]\n",
            "                [--augpipe {blit,geom,color,filter,noise,cutout,bg,bgc,bgcf,bgcfn,bgcfnc}]\n",
            "                [--cmethod {nocmethod,bcr,zcr,pagan,wgangp,auxrot,spectralnorm,shallowmap,adropout}]\n",
            "                [--dcap FLOAT] [--resume RESUME] [--freezed INT]\n",
            "\n",
            "Train a GAN using the techniques described in the paper\n",
            "\"Training Generative Adversarial Networks with Limited Data\".\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "\n",
            "general options:\n",
            "  --outdir DIR          Where to save the results (required)\n",
            "  --gpus INT            Number of GPUs to use (default: 1 gpu)\n",
            "  --snap INT            Snapshot interval (default: 50 ticks)\n",
            "  --seed INT            Random seed (default: 1000)\n",
            "  -n, --dry-run         Print training options and exit\n",
            "\n",
            "training dataset:\n",
            "  --data PATH           Training dataset path (required)\n",
            "  --res INT             Dataset resolution (default: highest available)\n",
            "  --mirror BOOL         Augment dataset with x-flips (default: false)\n",
            "  --mirrory BOOL        Augment dataset with y-flips (default: false)\n",
            "  --use-raw BOOL        Use raw image dataset, i.e. created from\n",
            "                        create_from_images_raw (default: False)\n",
            "\n",
            "metrics:\n",
            "  --metrics LIST        Comma-separated list or \"none\" (default: fid50k_full)\n",
            "  --metricdata PATH     Dataset to evaluate metrics against (optional)\n",
            "\n",
            "base config:\n",
            "  --cfg {auto,11gb-gpu,11gb-gpu-complex,24gb-gpu,24gb-gpu-complex,48gb-gpu,48gb-2gpu,stylegan2,paper256,paper512,paper1024,cifar,cifarbaseline,aydao}\n",
            "                        Base config (default: auto)\n",
            "  --lrate FLOAT         Override learning rate\n",
            "  --ttur BOOL           Use Two Time-Scale Update Rule (double learning rate\n",
            "                        for discriminator) (default: false)\n",
            "  --gamma FLOAT         Override R1 gamma\n",
            "  --nkimg INT           Override starting count\n",
            "  --kimg INT            Override training duration\n",
            "  --topk FLOAT          utilize top-k training\n",
            "\n",
            "discriminator augmentation:\n",
            "  --aug {noaug,ada,fixed,adarv}\n",
            "                        Augmentation mode (default: ada)\n",
            "  --p FLOAT             Specify augmentation probability for --aug=fixed\n",
            "  --target TARGET       Override ADA target for --aug=ada and --aug=adarv\n",
            "  --initstrength INITSTRENGTH\n",
            "                        Override ADA strength at start\n",
            "  --augpipe {blit,geom,color,filter,noise,cutout,bg,bgc,bgcf,bgcfn,bgcfnc}\n",
            "                        Augmentation pipeline (default: bgc)\n",
            "\n",
            "comparison methods:\n",
            "  --cmethod {nocmethod,bcr,zcr,pagan,wgangp,auxrot,spectralnorm,shallowmap,adropout}\n",
            "                        Comparison method (default: nocmethod)\n",
            "  --dcap FLOAT          Multiplier for discriminator capacity\n",
            "\n",
            "transfer learning:\n",
            "  --resume RESUME       Resume from network pickle (default: noresume)\n",
            "  --freezed INT         Freeze-D (default: 0 discriminator layers)\n",
            "\n",
            "examples:\n",
            "\n",
            "  # Train custom dataset using 1 GPU.\n",
            "  python train.py --outdir=~/training-runs --gpus=1 --data=~/datasets/custom\n",
            "\n",
            "  # Train class-conditional CIFAR-10 using 2 GPUs.\n",
            "  python train.py --outdir=~/training-runs --gpus=2 --data=~/datasets/cifar10c \\\n",
            "      --cfg=cifar\n",
            "\n",
            "  # Transfer learn MetFaces from FFHQ using 4 GPUs.\n",
            "  python train.py --outdir=~/training-runs --gpus=4 --data=~/datasets/metfaces \\\n",
            "      --cfg=paper1024 --mirror=1 --resume=ffhq1024 --snap=10\n",
            "\n",
            "  # Reproduce original StyleGAN2 config F.\n",
            "  python train.py --outdir=~/training-runs --gpus=8 --data=~/datasets/ffhq \\\n",
            "      --cfg=stylegan2 --res=1024 --mirror=1 --aug=noaug\n",
            "\n",
            "available base configs (--cfg):\n",
            "  auto           Automatically select reasonable defaults based on resolution\n",
            "                 and GPU count. Good starting point for new datasets.\n",
            "  stylegan2      Reproduce results for StyleGAN2 config F at 1024x1024.\n",
            "  paper256       Reproduce results for FFHQ and LSUN Cat at 256x256.\n",
            "  paper512       Reproduce results for BreCaHAD and AFHQ at 512x512.\n",
            "  paper1024      Reproduce results for MetFaces at 1024x1024.\n",
            "  cifar          Reproduce results for CIFAR-10 (tuned configuration).\n",
            "  cifarbaseline  Reproduce results for CIFAR-10 (baseline configuration).\n",
            "\n",
            "transfer learning source networks (--resume):\n",
            "  ffhq256        FFHQ trained at 256x256 resolution.\n",
            "  ffhq512        FFHQ trained at 512x512 resolution.\n",
            "  ffhq1024       FFHQ trained at 1024x1024 resolution.\n",
            "  celebahq256    CelebA-HQ trained at 256x256 resolution.\n",
            "  lsundog256     LSUN Dog trained at 256x256 resolution.\n",
            "  afhqcat512     AFHQ Cat trained at 512x512 resolution.\n",
            "  afhqdog512     AFHQ Dog trained at 512x512 resolution.\n",
            "  afhqwild512    AFHQ Wild trained at 512x512 resolution.\n",
            "  brecahad512    BreCaHAD trained at 512x512 resolution.\n",
            "  cifar10        CIFAR10 trained at 32x32 resolution.\n",
            "  metfaces512    MetFaces trained at 512x512 resolution.\n",
            "  <path or URL>  Custom network pickle.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9u3EmkP-sp2J",
        "outputId": "44fb6b38-0620-497a-b59c-62883246803b"
      },
      "source": [
        "training_path = project_path / 'training' / 'biked_small'\n",
        "if not training_path.is_dir():\n",
        "    %mkdir \"{training_path}\"\n",
        "\n",
        "snapshot_count = 2          # how often should the model generate samples and a .pkl file?\n",
        "mirrored = True             # should the images be mirrored left to right?\n",
        "mirroredY = False           # should the images be mirrored top to bottom?\n",
        "metric_list = None          # metrics? \n",
        "augs = 'bgc'                # which augments?\n",
        "\n",
        "resume_from = 'ffhq1024'    # see https://github.com/NVlabs/ffhq-dataset\n",
        "\n",
        "#!python \"{stylegan2_repo_path / 'train.py'}\" --outdir=\"{training_path}\" \\\n",
        "#    --data=\"{local_tfr_images_path}\" --resume=\"{resume_from}\" \\\n",
        "#    --snap={snapshot_count} --augpipe={augs} \\\n",
        "#    --mirror={mirrored} --mirrory={mirroredY} \\\n",
        "#    --metrics={metric_list} --dry-run\n",
        "\n",
        "!python \"{stylegan2_repo_path / 'train.py'}\" --outdir=\"{training_path}\" \\\n",
        "    --data=\"{local_tfr_images_path}\" \\\n",
        "    --snap={snapshot_count} --augpipe={augs} \\\n",
        "    --mirror={mirrored} --mirrory={mirroredY} \\\n",
        "    --metrics={metric_list} --kimg=1000"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/StyleGAN2-ADA/stylegan2-ada/train.py\", line 645, in <module>\n",
            "    main()\n",
            "  File \"/content/drive/MyDrive/StyleGAN2-ADA/stylegan2-ada/train.py\", line 637, in main\n",
            "    run_training(**vars(args))\n",
            "  File \"/content/drive/MyDrive/StyleGAN2-ADA/stylegan2-ada/train.py\", line 522, in run_training\n",
            "    training_loop.training_loop(**training_options)\n",
            "  File \"/content/drive/MyDrive/StyleGAN2-ADA/stylegan2-ada/training/training_loop.py\", line 252, in training_loop\n",
            "    tflib.run([G_train_op, data_fetch_op])\n",
            "  File \"/content/drive/MyDrive/StyleGAN2-ADA/stylegan2-ada/dnnlib/tflib/tfutil.py\", line 33, in run\n",
            "    return tf.get_default_session().run(*args, **kwargs)\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\", line 956, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\", line 1180, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\", line 1359, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\", line 1365, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\", line 1350, in _run_fn\n",
            "    target_list, run_metadata)\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\", line 1443, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PY0I-L_6yGjP"
      },
      "source": [
        "Training does periodically save the result and the model file `*.pkl` based on the provided `snapshot_count`. \n",
        "\n",
        "Once you think that the result is good enough or the FID starts to plateau, you can stop training and use the last saved `*.pkl` file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzU_cm8Tyiu6",
        "outputId": "8c196303-70a4-4ca4-ccf9-12ebea491085"
      },
      "source": [
        "%pip install opensimplex\n",
        "!python \"{stylegan2_repo_path / 'generate.py'}\" generate-images --help "
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opensimplex\n",
            "  Downloading opensimplex-0.3-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: opensimplex\n",
            "Successfully installed opensimplex-0.3\n",
            "Imageio: 'ffmpeg-linux64-v3.3.1' was not found on your computer; downloading it now.\n",
            "Try 1. Download from https://github.com/imageio/imageio-binaries/raw/master/ffmpeg/ffmpeg-linux64-v3.3.1 (43.8 MB)\n",
            "Downloading: 8192/45929032 bytes (0.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2342912/45929032 bytes (5.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5660672/45929032 bytes (12.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8904704/45929032 bytes (19.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12124160/45929032 bytes (26.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15409152/45929032 bytes (33.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b18456576/45929032 bytes (40.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b21741568/45929032 bytes (47.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b24846336/45929032 bytes (54.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28188672/45929032 bytes (61.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31268864/45929032 bytes (68.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b34545664/45929032 bytes (75.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b37789696/45929032 bytes (82.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b40755200/45929032 bytes (88.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b44294144/45929032 bytes (96.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45929032/45929032 bytes (100.0%)\n",
            "  Done\n",
            "File saved as /root/.imageio/ffmpeg/ffmpeg-linux64-v3.3.1.\n",
            "usage: generate.py generate-images [-h] --network NETWORK_PKL --seeds SEEDS\n",
            "                                   [--trunc TRUNCATION_PSI]\n",
            "                                   [--class CLASS_IDX] [--create-grid]\n",
            "                                   [--outdir DIR] [--save_vector] [--fixnoise]\n",
            "                                   [--jpg_quality JPG_QUALITY]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --network NETWORK_PKL\n",
            "                        Network pickle filename\n",
            "  --seeds SEEDS         List of random seeds\n",
            "  --trunc TRUNCATION_PSI\n",
            "                        Truncation psi (default: 0.5)\n",
            "  --class CLASS_IDX     Class label (default: unconditional)\n",
            "  --create-grid         Add flag to save the generated images in a grid\n",
            "  --outdir DIR          Root directory for run results (default: out)\n",
            "  --save_vector         also save vector in .npy format\n",
            "  --fixnoise            generate images using fixed noise (more accurate for\n",
            "                        interpolations)\n",
            "  --jpg_quality JPG_QUALITY\n",
            "                        Quality compression for JPG exports (1 to 95), keep\n",
            "                        default value to export as PNG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVZdQwdI5hI8",
        "outputId": "a7d1b38e-6613-4d2e-ba69-4975e18f614e"
      },
      "source": [
        "from numpy import random\n",
        "seed_init = random.randint(10000)\n",
        "nbr_images = 6\n",
        "\n",
        "#generation_from = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/metfaces.pkl'\n",
        "generation_from = training_path / '00000-biked_small_tfr-mirror-auto1-bgc-resumeffhq1024' / 'network-snapshot-000000.pkl'\n",
        "\n",
        "!python \"{stylegan2_repo_path / 'generate.py'}\" generate-images \\\n",
        "    --outdir=\"{project_path / 'out'}\" --trunc=0.7 \\\n",
        "    --seeds={seed_init}-{seed_init+nbr_images-1} \\\n",
        "    --network={str(generation_from)}"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading networks from \"/content/drive/MyDrive/StyleGAN2-ADA/training/biked_small/00000-biked_small_tfr-mirror-auto1-bgc-resumeffhq1024/network-snapshot-000000.pkl\"...\n",
            "Setting up TensorFlow plugin \"fused_bias_act.cu\": Loading... Done.\n",
            "Setting up TensorFlow plugin \"upfirdn_2d.cu\": Loading... Done.\n",
            "Generating image for seed 5717 (0/6) ...\n",
            "Generating image for seed 5718 (1/6) ...\n",
            "Generating image for seed 5719 (2/6) ...\n",
            "Generating image for seed 5720 (3/6) ...\n",
            "Generating image for seed 5721 (4/6) ...\n",
            "Generating image for seed 5722 (5/6) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4Z2cwOW9ywc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}